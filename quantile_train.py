from joblib import dump, load
import torch.utils.data as Data
import numpy as np
import torch
import torch.nn as nn
import time
import matplotlib
import matplotlib.pyplot as plt
# åŠ è½½æ•°æ®é›†
from module.CNN1DModel import CNN1DModel
# from module.CNNKAN import CNN1DKANModel
# from module.ChebyKAN import ChebyKAN
from module.DLinear import DLinear
# from module.GRUKAN import GRUKANModel
from module.KAN import KAN
from module.KANAttention import KANWithAttention
from module.LSTM import LSTMModel
# from module.LSTMKAN import LSTMKANModel
from module.MLP import MLP
# from module.TCNKAN import TCNKANModel
# from module.Transformer import TransformerModel
# from module.TransformerBiLSTM import TransformerBiLSTM
# from module.CompactAttentionKAN import CompactAttentionKAN

def dataloader(batch_size, workers=2):
    # è®­ç»ƒé›†
    train_set = load('train_xdata')
    train_label = load('train_ylabel')
    # æµ‹è¯•é›†
    val_set = load('val_xdata')
    val_label = load('val_ylabel')
    test_set = load('test_xdata')
    test_label = load('test_ylabel')
    # åŠ è½½æ•°æ®
    train_loader = Data.DataLoader(dataset=Data.TensorDataset(train_set, train_label),
                                   batch_size=batch_size, num_workers=workers, drop_last=True)
    val_loader = Data.DataLoader(dataset=Data.TensorDataset(val_set, val_label),
                                  batch_size=batch_size, num_workers=workers, drop_last=True)
    test_loader = Data.DataLoader(dataset=Data.TensorDataset(test_set, test_label),
                                  batch_size=batch_size, num_workers=workers, drop_last=True)
    return train_loader, val_loader, test_loader
def count_parameters(model):
    params = [p.numel() for p in model.parameters() if p.requires_grad]
    for item in params:
        print(f'{item:>6}')
    print(f'______\n{sum(params):>6}')
# -------- åˆ†ä½æ•°æŸå¤±ï¼ˆPinball Lossï¼‰ --------
class QuantileLoss(nn.Module):
    def __init__(self, quantiles):
        super().__init__()
        self.quantiles = quantiles  # e.g. [0.1, 0.5, 0.9]

    def forward(self, preds, target):
        """
        preds: [B, H, Q] æˆ– [B, H*Q] æˆ– [B, Q]
        target: [B, H] æˆ– [B, H, 1] æˆ– [B]
        """
        B = preds.size(0)
        Q = len(self.quantiles)

        # å¦‚æœ preds æ˜¯ [B, H*Q]ï¼Œreshape æˆ [B, H, Q]
        if preds.dim() == 2 and preds.size(1) % Q == 0:
            H = preds.size(1) // Q
            preds = preds.view(B, H, Q)

        # å¦‚æœ preds æ˜¯ [B, Q]ï¼Œå˜æˆ [B, 1, Q]
        elif preds.dim() == 2:
            preds = preds.unsqueeze(1)  # [B, 1, Q]

        # target reshape
        if target.dim() == 1:
            target = target.unsqueeze(1)  # [B, 1]
        if target.dim() == 3 and target.size(-1) == 1:
            target = target.squeeze(-1)  # [B, H]
        # ä¿è¯ target = [B, H]
        if target.dim() == 2:
            target = target.unsqueeze(-1)  # [B, H, 1]

        # ğŸ”‘ è¿™é‡Œç°åœ¨ä¸€å®šæ˜¯ preds: [B, H, Q], target: [B, H, 1]
        assert preds.shape[0] == target.shape[0] and preds.shape[1] == target.shape[1], \
            f"Shape mismatch: preds {preds.shape}, target {target.shape}"

        # pinball loss
        losses = []
        for i, q in enumerate(self.quantiles):
            errors = target - preds[..., i:i+1]  # [B, H, 1]
            loss_q = torch.max(q * errors, (q - 1) * errors)
            losses.append(loss_q.mean())

        return torch.stack(losses).mean()

def model_train(epochs, model, optimizer, loss_function, train_loader, val_loader, device):
    model = model.to(device)
    # æœ€ä½MSE
    minimum_mse = 1000.
    # æœ€ä½³æ¨¡å‹
    best_model = model

    train_mse = []     # è®°å½•åœ¨è®­ç»ƒé›†ä¸Šæ¯ä¸ªepochçš„ MSE æŒ‡æ ‡çš„å˜åŒ–æƒ…å†µ   å¹³å‡å€¼
    val_mse = []      # è®°å½•åœ¨æµ‹è¯•é›†ä¸Šæ¯ä¸ªepochçš„ MSE æŒ‡æ ‡çš„å˜åŒ–æƒ…å†µ   å¹³å‡å€¼

     # è®¡ç®—æ¨¡å‹è¿è¡Œæ—¶é—´
    start_time = time.time()
    for epoch in range(epochs):
         # è®­ç»ƒ
        model.train()
        quantiles = [0.25, 0.5, 0.75]
        loss_function = QuantileLoss(quantiles)
        train_mse_loss = []    #ä¿å­˜å½“å‰epochçš„MSE losså’Œ
        for seq, labels in train_loader:
            seq, labels = seq.to(device), labels.to(device)
            # æ¯æ¬¡æ›´æ–°å‚æ•°å‰éƒ½æ¢¯åº¦å½’é›¶å’Œåˆå§‹åŒ–
            optimizer.zero_grad()
            seq = seq.view(seq.size(0), -1)
            # å‰å‘ä¼ æ’­
            y_pred = model(seq)  #   torch.Size([16, 10])
            labels = labels.squeeze(-1)
            # print(y_pred.size())
            # print(labels.size())

            # æŸå¤±è®¡ç®—
            
            loss = loss_function(y_pred, labels)
            train_mse_loss.append(loss.item()) # è®¡ç®— MSE æŸå¤±
            # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
            loss.backward()
            optimizer.step()
            #     break
        # break
        # è®¡ç®—æ€»æŸå¤±
        train_av_mseloss = np.average(train_mse_loss) # å¹³å‡
        train_mse.append(train_av_mseloss)

        print(f'Epoch: {epoch+1:2} train_MSE-Loss: {train_av_mseloss:10.4f}')
        # æ¯ä¸€ä¸ªepochç»“æŸåï¼Œåœ¨éªŒè¯é›†ä¸ŠéªŒè¯å®éªŒç»“æœã€‚
        with torch.no_grad():
            # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            model.eval()
            val_mse_loss = []    #ä¿å­˜å½“å‰epochçš„MSE losså’Œ
            for data, label in val_loader:
                data, label = data.to(device), label.to(device)
                data = data.view(data.size(0),-1)
                pre = model(data)
                # è®¡ç®—æŸå¤±
                label = label.squeeze(-1)
                val_loss = loss_function(pre, label)
                val_mse_loss.append(val_loss.item())

            # è®¡ç®—æ€»æŸå¤±
            val_av_mseloss = np.average(val_mse_loss) # å¹³å‡
            val_mse.append(val_av_mseloss)
            print(f'Epoch: {epoch+1:2} val_MSE_Loss:{val_av_mseloss:10.4f}')
            # æ—©åœæœºåˆ¶
            if val_av_mseloss < minimum_mse:
                minimum_mse = val_av_mseloss
                patience_counter = 0
                torch.save(best_model, 'best_model_kan.pt')
            else:
                patience_counter += 1
                if patience_counter >= 5:
                    print(f'Early stopping at epoch {epoch+1}')
                    break
    # å¯è§†åŒ–
    # plt.plot(range(epochs), train_mse, color = 'b',label = 'train_MSE-loss')
    # plt.plot(range(epochs), val_mse, color = 'y',label = 'val_MSE-loss')
    # plt.legend()
    # plt.show()   #æ˜¾ç¤º lable
    print(f'min_MSE: {minimum_mse}')
if __name__ =="__main__":
    # å‚æ•°ä¸é…ç½®
    matplotlib.rc("font", family='Microsoft YaHei')
    torch.manual_seed(100)  # è®¾ç½®éšæœºç§å­ï¼Œä»¥ä½¿å®éªŒç»“æœå…·æœ‰å¯é‡å¤æ€§
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    batch_size = 64
    # åŠ è½½æ•°æ®
    train_loader, val_loader, test_loader = dataloader(batch_size)
    dump(test_loader,"test_loader")
    print(len(train_loader))
    print(len(test_loader))
     # å®šä¹‰æ¨¡å‹å‚æ•°
    input_size = 18*6
    # è¾“å…¥ä¸º 12 æ­¥
    # å®šä¹‰ ä¸€ä¸ªä¸‰å±‚çš„KAN ç½‘ç»œ
    hidden_dim1 = 128  # ç¬¬ä¸€å±‚éšè—å±‚ ç¥ç»å…ƒ 64ä¸ª
    hidden_dim2 = 64   # ç¬¬äºŒå±‚éšè—å±‚ ç¥ç»å…ƒ 32ä¸ª
    hidden_dim3 = 32
    output_size = 6# å¤šæ­¥é¢„æµ‹è¾“å‡º
    # Define model
    model = KANWithAttention([input_size, 32, 64, output_size*len([0.25, 0.5, 0.75])]) # è¾“å…¥ç‰¹å¾ä¸º12ï¼Œè¾“å‡ºå±‚æœ‰1ä¸ªç¥ç»å…ƒï¼Œç”¨äºå•ç‰¹å¾é¢„æµ‹
    loss_function = nn.MSELoss()  # loss
    learn_rate = 0.001
    optimizer = torch.optim.Adam(model.parameters(), learn_rate)  # ä¼˜åŒ–å™¨
    count_parameters(model)
    #  æ¨¡å‹è®­ç»ƒ
    epochs = 50
    model_train(epochs, model, optimizer, loss_function, train_loader, val_loader, device)